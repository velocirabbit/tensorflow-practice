{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const:0\", shape=(), dtype=float32) Tensor(\"Const_1:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "node1 = tf.constant(3.0, tf.float32)\n",
    "node2 = tf.constant(4.0)  # also tf.float32 implicitly\n",
    "print(node1, node2)  # Prints info about the nodes rather than node values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the nodes, we must run the computational graph within a session which encapsulates the control and state of the TensorFlow runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.0, 4.0]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "print(sess.run([node1, node2]))  # \"[3.0, 4.0]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build more complicated computations by combining Tensor nodes with \n",
    "operations (operations are also nodes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node3:  Tensor(\"Add:0\", shape=(), dtype=float32)\n",
      "sess.run(node3):  7.0\n"
     ]
    }
   ],
   "source": [
    "node3 = tf.add(node1, node2)\n",
    "print(\"node3: \", node3)\n",
    "print(\"sess.run(node3): \", sess.run(node3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TensorBoard utility displays a picture of the computational graph. A graph can be parameterized to accept external inputs, known as placeholders. A placeholder is a promise to provide a value later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = tf.placeholder(tf.float32)\n",
    "b = tf.placeholder(tf.float32)\n",
    "adder_node = a + b  # + provides a shortcut for tf.add(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a bit like defining a function or a lambda in which we define two input parameters (a and b) and then an operation on them.\n",
    "\n",
    "Use the feed_dict parameter to specify Tensors that provide concrete values to these placeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.5\n",
      "[ 3.  7.]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(adder_node, {a: 3, b: 4.5}))\n",
    "print(sess.run(adder_node, {a: [1, 3], b: [2, 4]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding another operation to make the computational graph more complex..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.5\n"
     ]
    }
   ],
   "source": [
    "add_and_triple = adder_node * 3.\n",
    "print(sess.run(add_and_triple, {a: 3, b: 4.5}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the model trainable, we need to be able to modify the graph to get new outputs with the same input. Variables allow us to add trainable parameters to a graph. They are constructed with a type and initial value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable([.3], tf.float32)\n",
    "b = tf.Variable([-.3], tf.float32)\n",
    "x = tf.placeholder(tf.float32)\n",
    "linear_model = W*x + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To initialize all the variables in a TensorFlow program, you must explicitly call a special operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "init is a handle to the TensorFlow subgraph that initializes all the global variables. Until we call sess.run, the variables are uninitialized\n",
    "\n",
    "Since x is a placeholder, we can evaluate linear_model for several values  of x simultaneously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.30000001  0.60000002  0.90000004]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(linear_model, {x: [1, 2, 3, 4]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the model on training data, we need a y placeholder to provide the desired values, and we need to write a loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.66\n"
     ]
    }
   ],
   "source": [
    "y = tf.placeholder(tf.float32)\n",
    "squared_deltas = tf.square(linear_model - y)\n",
    "loss = tf.reduce_sum(squared_deltas)\n",
    "print(sess.run(loss, {x: [1, 2, 3, 4], y: [0, -1, -2, -3]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables can be changed using operations like tf.assign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "fixW = tf.assign(W, [-1.])\n",
    "fixb = tf.assign(b, [1.])\n",
    "sess.run([fixW, fixb])\n",
    "print(sess.run(loss, {x: [1, 2, 3, 4], y: [0, -1, -2, -3]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow provides **optimizers** that slowly change each variable in order to minimize the loss function. The simplest optimizer is **gradient descent**. TensorFlow can automatically produce derivatives given only a description of the model using the function tf.gradients. For simplicity, optimizers typically do this for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-0.9999969], dtype=float32), array([ 0.99999082], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# tf.train.GradientDescentOptimizer takes a single parameter: learning_rate\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "sess.run(init)  # reset values to incorrect defaults\n",
    "for i in range(1000):\n",
    "    sess.run(train, {x: [1, 2, 3, 4], y: [0, -1, -2, -3]})\n",
    "    \n",
    "print(sess.run([W, b]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A completed, trainable linear regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: [-0.9999969], b: [ 0.99999082], loss: 5.69997e-11\n"
     ]
    }
   ],
   "source": [
    "# Model parameters\n",
    "W = tf.Variable([.3], tf.float32)\n",
    "b = tf.Variable([-.3], tf.float32)\n",
    "\n",
    "# Model input and output\n",
    "x = tf.placeholder(tf.float32)\n",
    "linear_model = W*x + b\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "# Loss function\n",
    "loss = tf.reduce_sum(tf.square(linear_model - y))  # sum of squared errors\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# Training data\n",
    "x_train = [1, 2, 3, 4]\n",
    "y_train = [0, -1, -2, -3]\n",
    "\n",
    "# Training loop\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)  # reset values to wrong\n",
    "for i in range(1000):\n",
    "    sess.run(train, {x: x_train, y: y_train})\n",
    "    \n",
    "# Evaluate training accuracy\n",
    "curr_w, curr_b, curr_loss = sess.run([W, b, loss], {x: x_train, y: y_train})\n",
    "print(\"W: %s, b: %s, loss: %s\" % (curr_w, curr_b, curr_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.contrib.learn\n",
    "\n",
    "tf.contrib.learn is a high-level TensorFlow library that simplifies the mechanics of machine learning, including the following:\n",
    "\n",
    "* running training loops\n",
    "* running evaluation loops\n",
    "* managing data sets\n",
    "* managing feeding\n",
    "\n",
    "tf.contrib.learn defines many common models.\n",
    "\n",
    "### Basic usage\n",
    "Basic linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\Andrew\\AppData\\Local\\Temp\\tmp6bc114jy\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_task_id': 0, '_save_checkpoints_secs': 600, '_environment': 'local', '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000023D7EACF128>, '_num_ps_replicas': 0, '_keep_checkpoint_every_n_hours': 10000, '_save_checkpoints_steps': None, '_is_chief': True, '_tf_random_seed': None, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_task_type': None, '_save_summary_steps': 100, '_evaluation_master': '', '_master': '', '_keep_checkpoint_max': 5}\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:From c:\\python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\head.py:1362: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into C:\\Users\\Andrew\\AppData\\Local\\Temp\\tmp6bc114jy\\model.ckpt.\n",
      "INFO:tensorflow:step = 1, loss = 3.75\n",
      "INFO:tensorflow:global_step/sec: 298.021\n",
      "INFO:tensorflow:step = 101, loss = 0.0938582\n",
      "INFO:tensorflow:global_step/sec: 278.097\n",
      "INFO:tensorflow:step = 201, loss = 0.0130046\n",
      "INFO:tensorflow:global_step/sec: 306.251\n",
      "INFO:tensorflow:step = 301, loss = 0.0032886\n",
      "INFO:tensorflow:global_step/sec: 305.311\n",
      "INFO:tensorflow:step = 401, loss = 0.000889084\n",
      "INFO:tensorflow:global_step/sec: 286.89\n",
      "INFO:tensorflow:step = 501, loss = 0.000101252\n",
      "INFO:tensorflow:global_step/sec: 304.38\n",
      "INFO:tensorflow:step = 601, loss = 3.73071e-05\n",
      "INFO:tensorflow:global_step/sec: 292.778\n",
      "INFO:tensorflow:step = 701, loss = 3.34508e-06\n",
      "INFO:tensorflow:global_step/sec: 319.477\n",
      "INFO:tensorflow:step = 801, loss = 7.24492e-07\n",
      "INFO:tensorflow:global_step/sec: 397.757\n",
      "INFO:tensorflow:step = 901, loss = 4.50483e-08\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into C:\\Users\\Andrew\\AppData\\Local\\Temp\\tmp6bc114jy\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 2.3334e-08.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:From c:\\python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\head.py:1362: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "INFO:tensorflow:Starting evaluation at 2017-03-28-01:25:41\n",
      "INFO:tensorflow:Finished evaluation at 2017-03-28-01:25:44\n",
      "INFO:tensorflow:Saving dict for global step 1000: global_step = 1000, loss = 4.60185e-08\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'global_step': 1000, 'loss': 4.6018481e-08}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Declare a list of features\n",
    "features = [tf.contrib.layers.real_valued_column(\"x\", dimension = 1)]\n",
    "\n",
    "# An estimator is the front end to invoke training (fitting) and evaluation\n",
    "# (inference). There are many predefined types.\n",
    "estimator = tf.contrib.learn.LinearRegressor(feature_columns = features)\n",
    "\n",
    "# TensorFlow provides many helper methods to read and set up data sets\n",
    "# We have to tell the function how many batches of data (num_epochs) we\n",
    "# want and how big each batch should be\n",
    "x = np.array([1., 2., 3., 4.])\n",
    "y = np.array([0., -1., -2., -3.])\n",
    "input_fn = tf.contrib.learn.io.numpy_input_fn({\"x\": x}, y, batch_size = 4,\n",
    "                                             num_epochs = 1000)\n",
    "\n",
    "# We can invoke 1000 training steps by invoking the 'fit' method and\n",
    "# passing in the training data set.\n",
    "estimator.fit(input_fn = input_fn, steps = 1000)\n",
    "\n",
    "# Here, we evaluate how well our model did\n",
    "estimator.evaluate(input_fn = input_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A custom model\n",
    "\n",
    "You can create custom models that aren't built into TensorFlow and still retain the high level abstraction of data set, feeding, training, etc. of tf.contrib.learn. This example will show how to implement an equivalent model to LinearRegressor.\n",
    "\n",
    "To define a custom model that works with tf.contrib.learn, use tf.contrib.learn.Estimator (tf.contrib.learn.LinearRegressor is actually a subclass of this). Instead of sub-classing Estimator, simply provide Estimator a function model_fn that tells tf.contrib.learn how it can evaluate predictions, training steps, and loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Declare list of features\n",
    "def model(features, labels, mode):\n",
    "    # Build a linear model and predict values\n",
    "    W = tf.get_variable(\"W\", [1], dtype = tf.float64)\n",
    "    b = tf.get_variable(\"b\", [1], dtype = tf.float64)\n",
    "    y = W*features['x'] + b\n",
    "    \n",
    "    # Loss sub-graph\n",
    "    loss = tf.reduce_sum(tf.square(y - labels))\n",
    "    \n",
    "    # Training subgraph\n",
    "    global_step = tf.train.get_global_step()\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "    train = tf.group(optimizer.minimize(loss),\n",
    "                    tf.assign_add(global_step, 1))\n",
    "    \n",
    "    # ModelFnOps connects subgraphs we built to the\n",
    "    # appropriate functionality\n",
    "    return tf.contrib.learn.ModelFnOps(\n",
    "        mode = mode, predictions = y,\n",
    "        loss = loss, train_op = train)\n",
    "\n",
    "estimator = tf.contrib.learn.Estimator(model_fn = model)\n",
    "\n",
    "# Define our data set\n",
    "x = np.array([1., 2., 3., 4.])\n",
    "y = np.array([0., -1., -2., -3.])\n",
    "input_fn = tf.contrib.learn.io.numpy_input_fn({\"x\": x}, y, 4, num_epochs = 1000)\n",
    "\n",
    "# Train\n",
    "estimator.fit(input_fn = input_fn, steps = 1000)\n",
    "\n",
    "# Evaluate our model\n",
    "print(estimator.evaluate(input_fn = input_fn, steps = 10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
